{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7309683,"sourceType":"datasetVersion","datasetId":4241490}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-02T15:49:47.291512Z","iopub.execute_input":"2024-01-02T15:49:47.291869Z","iopub.status.idle":"2024-01-02T15:50:35.526065Z","shell.execute_reply.started":"2024-01-02T15:49:47.291840Z","shell.execute_reply":"2024-01-02T15:50:35.524994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport glob\nimport os\nimport cv2\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-01-02T15:50:35.528152Z","iopub.execute_input":"2024-01-02T15:50:35.528941Z","iopub.status.idle":"2024-01-02T15:50:47.407333Z","shell.execute_reply.started":"2024-01-02T15:50:35.528907Z","shell.execute_reply":"2024-01-02T15:50:47.406169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# To Update label extraction function to assign distinct labels for different emotions\ndef extract_label_from_filename(filename):\n    emotion = filename.split('/')[-2]  # Extract the emotion from the path\n    # Assigning unique labels based on emotions (adjust labels as needed)\n    if emotion == \"angry\":\n        label = 0\n    elif emotion == \"disgust\":\n        label = 1\n    elif emotion == \"fear\":\n        label = 2\n    elif emotion == \"happy\":\n        label = 3\n    elif emotion == \"neutral\":\n        label = 4\n    elif emotion == \"sad\":\n        label = 5\n    elif emotion == \"surprise\":\n        label = 6\n    else:\n        label = -1  # Unknown label or error handling\n    return label\n\n# Function to load images from directory\ndef load_images_from_directory(directory):\n    images = []\n    labels = []\n    filenames = []\n\n    for root, dirs, files in os.walk(directory):\n        for file in files:\n            image_path = os.path.join(root, file)\n            try:\n                img = cv2.imread(image_path)\n                if img is not None:\n                    img = cv2.resize(img, (48, 48))\n                    images.append(img)\n                    label = extract_label_from_filename(image_path)\n                    labels.append(label)\n                    filenames.append(image_path)\n            except Exception as e:\n                print(f\"Error loading image {image_path}: {e}\")\n\n    return np.array(images), np.array(labels), np.array(filenames)\n\n# Defining directory paths\ntrain_directory = '/kaggle/input/affectnet-dataset/train'\ntest_directory = '/kaggle/input/affectnet-dataset/test'\n\n# Loads training images and labels\ntrain_images, train_labels, train_filenames = load_images_from_directory(train_directory)\n\n# Loads testing images and labels\ntest_images, test_labels, test_filenames = load_images_from_directory(test_directory)\n\n# Preprocessing the training images\ntrain_images_preprocessed = train_images / 255.0\n\n# Creating label mapping\nunique_labels = np.unique(np.concatenate((train_labels, test_labels)))\nlabel_mapping = {label: index for index, label in enumerate(unique_labels)}\n\n# Converting string labels to integers using the mapping\ntrain_labels_int = np.array([label_mapping[label] for label in train_labels])\ntest_labels_int = np.array([label_mapping[label] for label in test_labels])\n\n# Save preprocessed data\nnp.savez_compressed('preprocessed_data.npz', train_images=train_images_preprocessed, train_labels=train_labels_int, train_filenames=train_filenames,\n                   test_images=test_images, test_labels=test_labels_int, test_filenames=test_filenames)\n\n# Print information after preprocessing\nprint(\"Shape of preprocessed training images:\", train_images_preprocessed.shape)\nprint(\"Shape of training labels:\", train_labels_int.shape)\nprint(\"Shape of training filenames:\", train_filenames.shape)\nprint(\"Shape of preprocessed testing images:\", test_images.shape)\nprint(\"Shape of testing labels:\", test_labels_int.shape)\nprint(\"Shape of testing filenames:\", test_filenames.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-02T15:50:47.411174Z","iopub.execute_input":"2024-01-02T15:50:47.412035Z","iopub.status.idle":"2024-01-02T15:55:58.174155Z","shell.execute_reply.started":"2024-01-02T15:50:47.411999Z","shell.execute_reply":"2024-01-02T15:55:58.173047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define CNN model\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))  # Additional convolutional layer\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(7, activation='softmax'))\n    \n# Compiling the model\nmodel.compile(optimizer='rmsprop',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Preprocessing the images for training\ntrain_images_preprocessed = train_images / 255.0  # Normalize pixel values to be between 0 and 1\n\n# Train the model\nbatch_size = 64\nepochs = 50\n\n# Training the model\nhistory = model.fit(train_images_preprocessed, train_labels_int, batch_size=batch_size, epochs=epochs, verbose=1 )\nprint(\"Training complete.\")\n\n# Save the trained model\nmodel.save('emotion_detection_model.h5')\n#accuracy was 53.93% at 128,30","metadata":{"execution":{"iopub.status.busy":"2024-01-02T15:55:58.175340Z","iopub.execute_input":"2024-01-02T15:55:58.175654Z","iopub.status.idle":"2024-01-02T16:28:48.843468Z","shell.execute_reply.started":"2024-01-02T15:55:58.175627Z","shell.execute_reply":"2024-01-02T16:28:48.842557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(train_labels)\n\n# Converting labels to integers for training set\ntrain_labels_int = label_encoder.transform(train_labels)\n\n# Convert labels to integers for test set\ntest_labels_int = label_encoder.transform(test_labels)","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:29:04.096968Z","iopub.execute_input":"2024-01-02T16:29:04.097362Z","iopub.status.idle":"2024-01-02T16:29:04.441880Z","shell.execute_reply.started":"2024-01-02T16:29:04.097329Z","shell.execute_reply":"2024-01-02T16:29:04.440866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming you have the 'test_images' variable from previous steps\ntest_images_preprocessed = test_images / 255.0  # Normalize pixel values to be between 0 and 1\n\n# Assuming the model is already defined and trained\n# Evaluate the model on the test set\ntest_loss, test_accuracy = model.evaluate(test_images_preprocessed, test_labels_int)\nprint(f'Test Accuracy: {test_accuracy * 100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:29:08.349381Z","iopub.execute_input":"2024-01-02T16:29:08.350255Z","iopub.status.idle":"2024-01-02T16:29:12.793058Z","shell.execute_reply.started":"2024-01-02T16:29:08.350198Z","shell.execute_reply":"2024-01-02T16:29:12.792052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#accuracy was 53.93% at 128,30\n#accuracy was 56.78% at 64,50","metadata":{"execution":{"iopub.status.busy":"2024-01-02T16:30:12.293805Z","iopub.execute_input":"2024-01-02T16:30:12.294224Z","iopub.status.idle":"2024-01-02T16:30:12.299465Z","shell.execute_reply.started":"2024-01-02T16:30:12.294193Z","shell.execute_reply":"2024-01-02T16:30:12.298269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define CNN model\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))  # Additional convolutional layer\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dense(7, activation='softmax'))\n    \n# Compile the model\nfrom keras.optimizers import Adagrad\n\noptimizer = Adagrad(learning_rate=0.1)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n# Preprocess the images for training\ntrain_images_preprocessed = train_images / 255.0  # Normalize pixel values to be between 0 and 1\n\n# Train the model\nbatch_size = 64\nepochs = 30\n\n# Train the model\nhistory = model.fit(train_images_preprocessed, train_labels_int, batch_size=batch_size, epochs=epochs, verbose=1, validation_split = 0.2 )\nprint(\"Training complete.\")\n\n# Save the trained model\nmodel.save('emotion_detection_model.h5')\n#accuracy was 51.37%\n#Validation Loss: 5.2653\n#Validation Accuracy: 48.47%","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The model achieves almost perfect training accuracy (99.64%), suggesting it fits the training data well.\n#Decreasing training loss: The training loss steadily decreases throughout the training process, indicating efficient learning.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define CNN model with dropout layers\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.2))  # Dropout after first convolutional block\n\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.2))  # Dropout after second convolutional block\n\nmodel.add(layers.Conv2D(128, (3, 3), activation='relu'))  # Additional convolutional layer\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Dropout(0.2))  # Dropout after third convolutional block\n\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(128, activation='relu'))\nmodel.add(layers.Dropout(0.4))  # Dropout before the final dense layer\n\nmodel.add(layers.Dense(7, activation='softmax'))\n\n    \n# Compile the model\nfrom keras.optimizers import Adagrad\n\noptimizer = Adagrad(learning_rate=0.1)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n# Preprocess the images for training\ntrain_images_preprocessed = train_images / 255.0  # Normalize pixel values to be between 0 and 1\n\n# Train the model\nbatch_size = 32\nepochs = 30\n\n# Train the model\nhistory = model.fit(train_images_preprocessed, train_labels_int, batch_size=batch_size, epochs=epochs, verbose=1, validation_split = 0.2 )\nprint(\"Training complete.\")\n\n# Save the trained model\nmodel.save('emotion_detection_model.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#225/225 [==============================] - 3s 15ms/step - loss: 1.2716 - accuracy: 0.5524\n#Test Accuracy: 55.24%","metadata":{"execution":{"iopub.status.busy":"2024-01-02T17:41:14.911865Z","iopub.execute_input":"2024-01-02T17:41:14.912265Z","iopub.status.idle":"2024-01-02T17:41:14.941748Z","shell.execute_reply.started":"2024-01-02T17:41:14.912232Z","shell.execute_reply":"2024-01-02T17:41:14.940850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Validation Loss: 1.5406\n#Validation Accuracy: 54.02%","metadata":{"execution":{"iopub.status.busy":"2024-01-02T17:41:37.194111Z","iopub.execute_input":"2024-01-02T17:41:37.194547Z","iopub.status.idle":"2024-01-02T17:41:37.199319Z","shell.execute_reply.started":"2024-01-02T17:41:37.194511Z","shell.execute_reply":"2024-01-02T17:41:37.198022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}